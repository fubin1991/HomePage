 .<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <title>Fu Bin's HomePage</title>
    <link rel="stylesheet" type="text/css" href="../../Page_site.css">
	<link rel="stylesheet" type="text/css" href="Basic_Element_in_NN_set.css">
  </head> 

  <body>
   <div id="allcontent">
	<nav>
	<ul>
		<li ><a href="../../index.html">Homepage</a></li>
		<!-- <li><a href="../blog/blog.html">Blog</a></li> -->
		<li class="selected"><a href="../research.html">Research</a></li>
		<li><a target="_blank" href="http://www.physics.hku.hk/~phys4150/">Course</a></li>
	</ul>
	</nav>
	
    <h1 id="research">Basic Element in Neural Network Reading List</h1>
	
	<section id="research">
	
	<div>
	
	<h2>Network Structure</h2>
	 <p>
		<ul>
		<li><em>NIN:</em> Network in network. (2014)</li>
		<li><em>DSN:</em> Deeply supervised nets. (2015)</li>
		<li>Semi-supervised learning with ladder networks. (2015)</li>
		<li>Deconstructing the ladder network architecture. (2015)</li>
		<li><em>DFNs:</em> Deeply-fused nets. (2016)</li>
		<li><em>Highway Networks:</em> Training very deep networks. (2015)</li>
		<li><em>ResNet:</em> Deep residual learning for image recognition. (2016)</li>
		<li>Deep networks with stochastic depth. (2016) <br>
			This paper show that many layers of ResNet contribute very little and can in fact be randomly dropped
			during training. Therefore this paper shortens ResNets by randomly dropping layers during training to allow better information
			and gradient flow.</li>
		<li>Bridging the gaps between residual learning, recurrent neural networks and visual cortex</li>
		<li><em>Fractalnet:</em> Ultra-deep neural networks without residuals. (2016) <br>
			This paper repeatedly combine several parallel layer sequences with different number of convolutional
			blocks to obtain a large nominal depth, while maintaining many short paths in the network.</li>
		<li>Resnet in resnet: Generalizing residual architectures.</li>
		<li>Wide residual networks.</li>
		<li>Residual Networks Behave Like Ensembles of Relatively Shallow Networks. (2016) </li>
		<li>Wider or Deeper: Revisiting the ResNet Model for Visual Recognition. (2016) </li>
		<li><em>DenseNet:</em> Densely Connected Convolutional Networks. (2017) <br>
			This paper brings up a new network structure. For each layer, the feature-maps of all preceding layers are
			used as inputs, and its own feature-maps are used as inputs into all subsequent layers.</li>
		<li><em>DRNs:</em> Dilated Residual Networks. (2017) </li>
		</ul>
	 </p>
	
	<h2>Convolution</h2>
	 <p>
		<ul>
		<li><em>Normalized convolution:</em> Normalized and differential convolution. (1993) <br>
			The idea of normalized convolution is to “focus” the convolution operator on the part of the input that truly describes the input signal,
			avoiding the interpolation of noise or missing information.</li>
		<li><em>Dilated Conv:</em> Multi-scale contest aggregation by dilated convolutions. </li>
		<li><em>Dilated Conv:</em> Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs. </li>
		<li><em>Deconvolution:</em> Adaptive deconvolutional networks for mid and high level feature learning. (2011) </li>
		<li><em>Deconvolution:</em> Visualizing and understanding convolutional networks. (2014) </li>
		<li><em>Flattened convolutional:</em> Flattened Convolutional Neural Networks for Feedforward Acceleration. (2014) </li>
		<li><em>PixelDCL:</em> Pixel Deconvolutional Networks. (2017) <br>
			One of the key limitations of deconvolutional operations is that they result in the so-called checkerboard problem. 
			This is caused by the fact that no direct relationship exists among adjacent pixels on the output feature map. 
			To address this problem, we propose the pixel deconvolutional layer (PixelDCL) to establish direct relationships among adjacent 
			pixels on the up-sampled feature map. <br>
			This PixelDCL is very similiar with DenseNet.</li>
		</ul>
	 </p>
	 
	 <h2>Other</h2>
	 <p>
		<ul>
		<li><em>DropOut:</em> Improving neural networks by preventing co-adaptation of feature detectors. (2012) </li>
		<li><em>Batch Normalization:</em> Accelerating Deep Network Training by Reducing Internal Covariate Shift. (2015) </li>
		</ul>
	 </p>
	 
	 
	</div>
	

	</section>
	
	
	
    <footer>
	<p>
	e-mail: fubin1991 at outlook.com 
	</p>
	<p>
      &copy; 2017, Fu Bin
	</p>
    </footer>

  </div>
  </body>
</html>


