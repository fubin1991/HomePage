 .<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <title>Fu Bin's HomePage</title>
    <link rel="stylesheet" type="text/css" href="../../Page_site.css">
	<link rel="stylesheet" type="text/css" href="Basic_Element_in_NN_set.css">
  </head> 

  <body>
   <div id="allcontent">
	<nav>
	<ul>
		<li ><a href="../../index.html">Homepage</a></li>
		<!-- <li><a href="../blog/blog.html">Blog</a></li> -->
		<li class="selected"><a href="../research.html">Research</a></li>
		<li><a target="_blank" href="http://www.physics.hku.hk/~phys4150/">Course</a></li>
	</ul>
	</nav>
	
    <h1 id="research">Basic Element in Neural Network Reading List</h1>
	
	<section id="research">
	
	<div>
	
	<h2>Network Structure</h2>
	 <p>
		<ul>
		<li><em>NIN:</em> Network in network. (2014)</li>
		<li><em>DSN:</em> Deeply supervised nets. (2015)</li>
		<li>Semi-supervised learning with ladder networks. (2015)</li>
		<li>Deconstructing the ladder network architecture. (2015)</li>
		<li><em>DFNs:</em> Deeply-fused nets. (2016)</li>
		<li><em>Highway Networks:</em> Training very deep networks. (2015)</li>
		<li><em>ResNet:</em> Deep residual learning for image recognition. (2016)</li>
		<li>Deep networks with stochastic depth. (2016) <br>
			This paper show that many layers of ResNet contribute very little and can in fact be randomly dropped
			during training. Therefore this paper shortens ResNets by randomly dropping layers during training to allow better information
			and gradient flow.</li>
		<li>Bridging the gaps between residual learning, recurrent neural networks and visual cortex</li>
		<li><em>Fractalnet:</em> Ultra-deep neural networks without residuals. (2016) <br>
			This paper repeatedly combine several parallel layer sequences with different number of convolutional
			blocks to obtain a large nominal depth, while maintaining many short paths in the network.</li>
		<li>Resnet in resnet: Generalizing residual architectures.</li>
		<li>Wide residual networks.</li>
		<li><em>DenseNet:</em> Densely Connected Convolutional Networks. (2017) <br>
			This paper brings up a new network structure. For each layer, the feature-maps of all preceding layers are
			used as inputs, and its own feature-maps are used as inputs into all subsequent layers.</li>
		<li><em>DRNs:</em> Dilated Residual Networks. (2017) </li>
		</ul>
	 </p>
	
	<h2>Convolution</h2>
	 <p>
		<ul>
		<li><em>Normalized convolution:</em> Normalized and differential convolution. (1993) <br>
			The idea of normalized convolution is to “focus” the convolution operator on the part of the input that truly describes the input signal,
			avoiding the interpolation of noise or missing information.</li>
		<li><em>Dilated Conv:</em> Multi-scale contest aggregation by dilated convolutions. </li>
		<li><em>Dilated Conv:</em> Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs. </li>
		<li><em>Deconvolution:</em> Adaptive deconvolutional networks for mid and high level feature learning. (2011) </li>
		<li><em>Deconvolution:</em> Visualizing and understanding convolutional networks. (2014) </li>
		<li><em>Flattened convolutional:</em> Flattened Convolutional Neural Networks for Feedforward Acceleration. (2014) </li>
		</ul>
	 </p>
	 
	 <h2>Other</h2>
	 <p>
		<ul>
		<li><em>DropOut:</em> Improving neural networks by preventing co-adaptation of feature detectors. (2012) </li>
		<li><em>Batch Normalization:</em> Accelerating Deep Network Training by Reducing Internal Covariate Shift. (2015) </li>
		</ul>
	 </p>
	 
	 
	</div>
	
	<!--
	<div>
	<h2>Fast Multipole Method</h2>
	 <p>
	    FMM is one of ten most important algorithm in 20 century, it reduce the calculate cost from 2N to N and is extremely efficient for large N.
	 </p>
	 <p>
	    <em>The important Reading Materials for FMM:</em> <br>
		<ul>
		<li> V. Rokhlin, Rapid Solution of Integral Equations of Classical Potential Theory, Journal of Computational Physics, 60, 187-207, 1983. </li>
		<li> Leslie Greengard, The Rapid Evaluation of Potential Fields in Particle System. </li>
		<li> J. Carrier, L. Greengard, and V. Rokhlin. A Fast Adaptive Multipole Algorithm for Particle Simulations, SIAM J. STAT. COMPUT. Vol. 9, No. 4, 1988.</li>
		<li> A. Dutt, M. Gu and V. Rokhlin, Fast Algorithms for Polynomial Interpolation, Integration, and Differentiation, SIAM J. Numer. Anal, Vol. 33, No. 5, pp. 1689-1711, 1996. </li>
		<li> T. Hrycak and V. Rokhlin, An Improved Fast Multipole Algorithm for Potential Fields, SIAM J. Sci. Comput, Vol. 19, No. 6, pp. 1804-1826, 1998. </li>
	    </ul>
	 </p>
	 <p>
	    I write a note for <a target="_blank" href="Fast Multipole Method.pdf">Fast Multipole Method</a> to explain most important details about FMM method. If you are confused by FMM method, this note may help you to catch the most important conceptions.
	 </p>
	 <p>
	    <em>For our paper:</em> <br>
		<a target="_blank" href="https://arxiv.org/abs/1602.01638">Order O(1) algorithm for first-principles transient current through open quantum systems</a> <br>
		You also can refer this note for details: <br>
		<a target="_blank" href="evaluation operator.pdf">Fast Calculate for Evolution Operators</a>
	 </p>
	</div>
	-->

	</section>
	
	
	
    <footer>
	<p>
	e-mail: fubin1991 at outlook.com 
	</p>
	<p>
      &copy; 2017, Fu Bin
	</p>
    </footer>

  </div>
  </body>
</html>


